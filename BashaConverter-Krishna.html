<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>BhashaConvert ¬∑ Advanced AI Translation</title>
  <!-- Tailwind CSS -->
  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          fontFamily: { sans: ["Inter", "ui-sans-serif", "system-ui"] },
          colors: { neon: { 500: '#6ef3ff' }, accent: { 600: '#8b5cf6' } }
        }
      }
    };
  </script>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
  <!-- External libs (free, client-side) -->
  <script src="https://unpkg.com/mammoth/mammoth.browser.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/2.13.216/pdf.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js"></script>
  <script src="https://unpkg.com/tesseract.js@v4.0.2/dist/tesseract.min.js"></script>
  <!-- PDF generation library -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jspdf/2.5.1/jspdf.umd.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.4.1/html2canvas.min.js"></script>
  <!-- Audio processing -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/wavesurfer.js/7.7.3/wavesurfer.min.js"></script>
  <style>
    .nav-active { border-bottom: 3px solid #8b5cf6; color: #8b5cf6; }
    .nav-btn { border-bottom: 3px solid transparent; transition: color .2s, border-color .2s; }
    .panel { display: none; }
    .panel.active { display: block; }
    .glass {
      background: rgba(255,255,255,.09);
      box-shadow: 0 2px 12px rgba(0,0,0,.08);
      backdrop-filter: blur(10px);
    }
    .card {
      background: rgba(255,255,255,.05);
      border: 1px solid rgba(255,255,255,.1);
      border-radius: 1rem;
      padding: 1.5rem;
      margin-bottom: 2rem;
    }
    .rounded-xl { border-radius: 1rem; }
    .input, .btn, .btn-primary {
      border-radius: 6px;
      border: none;
      transition: all 0.2s ease;
    }
    .input { 
      padding: 12px; 
      background: rgba(15, 24, 58, 0.8); 
      color: #fff; 
      width: 100%; 
      border: 1px solid rgba(255,255,255,0.1);
    }
    .input:focus {
      outline: none;
      border-color: #8b5cf6;
      box-shadow: 0 0 0 3px rgba(139, 92, 246, 0.1);
    }
    .btn { 
      background: rgba(36, 43, 73, 0.8); 
      color: #fff; 
      padding: 10px 20px; 
      cursor: pointer;
      border: 1px solid rgba(255,255,255,0.1);
    }
    .btn-primary { 
      background: linear-gradient(135deg, #8b5cf6, #6ef3ff); 
      color: #fff; 
      padding: 10px 20px; 
      cursor: pointer;
      font-weight: 600;
    }
    .btn:hover { 
      background: rgba(68, 75, 105, 0.9); 
      transform: translateY(-1px);
    }
    .btn-primary:hover { 
      background: linear-gradient(135deg, #6ef3ff, #8b5cf6); 
      color: #071029; 
      transform: translateY(-1px);
    }
    
    /* Video panel specific styles */
    .video-container {
      background: rgba(0,0,0,0.3);
      border-radius: 1rem;
      padding: 1rem;
      margin: 1rem 0;
    }
    
    .subtitle-segment {
      background: rgba(139, 92, 246, 0.1);
      border: 1px solid rgba(139, 92, 246, 0.3);
      border-radius: 0.5rem;
      padding: 0.75rem;
      margin: 0.5rem 0;
    }
    
    .subtitle-segment[contenteditable="true"]:focus {
      outline: none;
      border-color: #6ef3ff;
      background: rgba(110, 243, 255, 0.1);
    }
    
    /* File upload styling */
    input[type="file"] {
      background: rgba(15, 24, 58, 0.8);
      border: 1px solid rgba(255,255,255,0.1);
      border-radius: 0.5rem;
      padding: 0.5rem;
      color: white;
      width: 100%;
    }
    
    /* Loading states */
    .loading {
      opacity: 0.6;
      pointer-events: none;
    }
    
    /* Success/Error message styling */
    .message {
      padding: 0.75rem;
      border-radius: 0.5rem;
      margin: 0.5rem 0;
      font-size: 0.875rem;
    }
    
    .message.success {
      background: rgba(34, 197, 94, 0.1);
      border: 1px solid rgba(34, 197, 94, 0.3);
      color: #22c55e;
    }
    
    .message.warning {
      background: rgba(245, 158, 11, 0.1);
      border: 1px solid rgba(245, 158, 11, 0.3);
      color: #f59e0b;
    }
    
    .message.error {
      background: rgba(239, 68, 68, 0.1);
      border: 1px solid rgba(239, 68, 68, 0.3);
      color: #ef4444;
    }
    
    /* Audio visualization */
    .audio-visualizer {
      background: rgba(0,0,0,0.2);
      border-radius: 0.5rem;
      padding: 1rem;
      margin: 1rem 0;
    }
    
    /* Chat interface */
    .chat-interface {
      position: fixed;
      top: 50%;
      right: 20px;
      transform: translateY(-50%);
      width: 350px;
      max-height: 80vh;
      background: rgba(15, 24, 58, 0.95);
      border: 1px solid rgba(139, 92, 246, 0.3);
      border-radius: 1rem;
      backdrop-filter: blur(20px);
      z-index: 1000;
      display: none;
    }
    
    .chat-interface.active {
      display: block;
    }
    
    .chat-header {
      background: linear-gradient(135deg, #8b5cf6, #6ef3ff);
      padding: 1rem;
      border-radius: 1rem 1rem 0 0;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }
    
    .chat-toggle {
      position: fixed;
      top: 20px;
      right: 20px;
      background: linear-gradient(135deg, #8b5cf6, #6ef3ff);
      border: none;
      color: white;
      padding: 0.75rem;
      border-radius: 50%;
      cursor: pointer;
      z-index: 1001;
      box-shadow: 0 4px 15px rgba(139, 92, 246, 0.3);
    }
    
    /* Responsive improvements */
    @media (max-width: 768px) {
      .card {
        padding: 1rem;
      }
      .grid {
        grid-template-columns: 1fr !important;
      }
      .chat-interface {
        width: 90vw;
        right: 5vw;
      }
    }
  </style>
</head>
<body class="min-h-screen bg-[#071029] text-white font-sans p-0">
  <!-- Chat Toggle Button -->
  <button class="chat-toggle" onclick="toggleChat()" title="Real-time Chat Translator">
    <div class="flex items-center justify-center">
      <svg class="w-6 h-6" fill="currentColor" viewBox="0 0 24 24">
        <path d="M20 2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h4l4 4 4-4h4c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2z"/>
        <path d="M9 11h6v2H9z"/>
        <path d="M9 7h6v2H9z"/>
      </svg>
    </div>
  </button>

  <!-- Chat Interface -->
  <div id="chatInterface" class="chat-interface">
    <div class="chat-header">
      <h3 class="font-semibold">üí¨ Real-time Chat Translator</h3>
      <button onclick="toggleChat()" class="text-white hover:text-gray-200">‚úï</button>
    </div>
    <div class="p-4">
      <div class="mb-3">
        <select id="chatFromLang" class="input text-sm">
          <option value="auto">Auto Detect</option>
          <option value="en">English</option>
          <option value="hi">Hindi</option>
          <option value="mr">Marathi</option>
          <option value="gu">Gujarati</option>
          <option value="ta">Tamil</option>
          <option value="bn">Bengali</option>
          <option value="te">Telugu</option>
          <option value="kn">Kannada</option>
        </select>
        <select id="chatToLang" class="input text-sm mt-2">
          <option value="hi">Hindi</option>
          <option value="mr">Marathi</option>
          <option value="gu">Gujarati</option>
          <option value="ta">Tamil</option>
          <option value="bn">Bengali</option>
          <option value="en">English</option>
          <option value="te">Telugu</option>
          <option value="kn">Kannada</option>
        </select>
      </div>
      <textarea id="chatInput" class="input text-sm min-h-[80px]" placeholder="Type your message..."></textarea>
      <button class="btn-primary w-full mt-2 text-sm" onclick="translateChat()">Translate & Send</button>
      <div id="chatHistory" class="mt-3 max-h-[300px] overflow-y-auto text-sm">
        <div class="text-white/60 text-xs">Chat messages will appear here...</div>
      </div>
      <div class="flex gap-2 mt-3">
        <button class="btn text-xs" onclick="clearChat()">Clear</button>
        <button class="btn text-xs" onclick="downloadChat()">Download</button>
      </div>
    </div>
  </div>

  <div class="max-w-6xl mx-auto px-4 pt-8 pb-6">
    <header class="mb-8 flex items-center justify-between">
      <h1 class="text-3xl font-bold tracking-tight">Bhasha-Converter <span class="text-accent-600">Krishna</span></h1>
      <div class="text-sm text-white/70">üöÄ Advanced AI Translation ‚Ä¢ 8 Languages ‚Ä¢ Real-time Processing ‚Ä¢ Free Forever</div>
    </header>
    
    <!-- Competitive Features Banner -->
    <div class="mb-6 p-4 bg-gradient-to-r from-accent-600/20 to-neon-500/20 rounded-xl border border-accent-600/30">
      <div class="flex items-center justify-between">
        <div class="flex items-center space-x-4">
          <div class="flex items-center space-x-2">
            <span class="text-green-400">‚úì</span>
            <span class="text-sm">No API Keys Required</span>
          </div>
          <div class="flex items-center space-x-2">
            <span class="text-green-400">‚úì</span>
            <span class="text-sm">100% Free</span>
          </div>
          <div class="flex items-center space-x-2">
            <span class="text-green-400">‚úì</span>
            <span class="text-sm">Privacy First</span>
          </div>
        </div>
        <div class="text-right">
          <div class="text-xs text-white/60">Unique Features</div>
          <div class="text-sm font-semibold">PDF Generation ‚Ä¢ OCR ‚Ä¢ Audio Analysis</div>
        </div>
      </div>
    </div>

    <!-- Navigation Bar -->
    <nav class="mb-8 border-b border-white/15 flex space-x-4">
      <button class="nav-btn text-lg py-2 px-4 nav-active" data-panel="text-panel">Text</button>
      <button class="nav-btn text-lg py-2 px-4" data-panel="audio-panel">Audio</button>
      <button class="nav-btn text-lg py-2 px-4" data-panel="document-panel">Document</button>
      <button class="nav-btn text-lg py-2 px-4" data-panel="video-panel">Video</button>
    </nav>

    <!-- Panels -->
    <div id="text-panel" class="panel active">
      <!-- Text translation panel -->
      <section class="card">
        <h2 class="text-xl font-semibold mb-3">üöÄ Advanced Text Translation Engine</h2>
        <div class="grid lg:grid-cols-2 gap-6">
          <div>
            <label class="text-sm text-white/70">Source Language</label>
            <select id="fromLang" class="input mt-1">
              <option value="auto">Auto Detect</option>
              <option value="en">English</option>
              <option value="hi">Hindi</option>
              <option value="mr">Marathi</option>
              <option value="gu">Gujarati</option>
              <option value="ta">Tamil</option>
              <option value="bn">Bengali</option>
              <option value="te">Telugu</option>
              <option value="kn">Kannada</option>
            </select>
            <label class="text-sm text-white/70 mt-4">Domain (context)</label>
            <select id="domain" class="input mt-1">
              <option>General</option>
              <option>Legal</option>
              <option>Medical</option>
              <option>Education</option>
              <option>eCommerce</option>
              <option>Technical</option>
              <option>Creative</option>
            </select>
            <label class="text-sm text-white/70 mt-4">Input Text</label>
            <textarea id="inputText" class="input mt-1 min-h-[120px]" placeholder="Type or paste text here..."></textarea>
            <div class="flex gap-2 mt-4 flex-wrap">
              <button class="btn-primary" onclick="translateText()">Translate ‚ñ∂</button>
              <button class="btn" onclick="grammarFix()">Grammar Fix</button>
              <button class="btn" onclick="rewriteStyle('formal')">Rewrite (Formal)</button>
              <button class="btn" onclick="rewriteStyle('informal')">Rewrite (Informal)</button>
            </div>
            <div class="mt-4 flex gap-3 items-center flex-wrap">
              <button class="btn" id="micBtn" onclick="startListening()">üéô Start Mic</button>
              <button class="btn" onclick="stopListening()">‚èπ Stop Mic</button>
              <button class="btn" onclick="clearInput()">Clear</button>
            </div>
            <div class="mt-3 text-xs text-white/60">
              ‚ú® Advanced AI translation with context awareness ‚Ä¢ Grammar correction ‚Ä¢ Style adaptation
            </div>
          </div>
          <div>
            <label class="text-sm text-white/70">Target Language</label>
            <select id="toLang" class="input mt-1">
              <option value="hi">Hindi</option>
              <option value="mr">Marathi</option>
              <option value="gu">Gujarati</option>
              <option value="ta">Tamil</option>
              <option value="bn">Bengali</option>
              <option value="en">English</option>
              <option value="te">Telugu</option>
              <option value="kn">Kannada</option>
            </select>
            <label class="text-sm text-white/70 mt-4">Tone</label>
            <div class="flex gap-2 mt-1">
              <button class="btn" onclick="setTone('neutral')">Neutral</button>
              <button class="btn" onclick="setTone('formal')">Formal</button>
              <button class="btn" onclick="setTone('informal')">Informal</button>
            </div>
            <label class="text-sm text-white/70 mt-4">Output</label>
            <textarea id="outputText" class="input mt-1 min-h-[120px]" readonly></textarea>
            <div class="flex gap-2 mt-4">
              <button class="btn" onclick="copyOut()">Copy</button>
              <button class="btn" onclick="speakText()">üîä Speak</button>
              <button class="btn" onclick="downloadTxt('translation.txt', document.getElementById('outputText').value)">Download</button>
            </div>
          </div>
        </div>
      </section>
    </div>

    <div id="audio-panel" class="panel">
      <!-- Voice & Audio -->
      <section class="card">
        <h2 class="text-xl font-semibold mb-3">üéµ Advanced Voice & Audio Translation</h2>
        <div class="grid lg:grid-cols-2 gap-6">
          <div>
            <label class="text-sm text-white/70">Live Speech ‚Üí Translate</label>
            <div class="mt-2 glass rounded-xl p-3">
              <div class="mb-3">
                <label class="text-sm text-white/70">Live Speech Language</label>
                <select id="liveSpeechLang" class="input mt-1">
                  <option value="auto">Auto Detect</option>
                  <option value="en">English</option>
                  <option value="hi">Hindi</option>
                  <option value="mr">Marathi</option>
                  <option value="gu">Gujarati</option>
                  <option value="ta">Tamil</option>
                  <option value="bn">Bengali</option>
                  <option value="te">Telugu</option>
                  <option value="kn">Kannada</option>
                </select>
              </div>
              <div id="liveTranscript" class="text-sm text-white/80 min-h-[80px]">Transcript will appear here when you use the mic.</div>
              <div class="mt-3 flex gap-2">
                <button class="btn-primary" onclick="startTranslateLive()">Start Live Translate</button>
                <button class="btn" onclick="stopTranslateLive()">Stop</button>
              </div>
            </div>
            <div class="mt-4">
              <label class="text-sm text-white/70">Upload Audio (MP3 / WAV)</label>
              <input type="file" id="audioFile" accept="audio/*" class="mt-2" onchange="handleAudioUpload(event)" />
              <div class="mt-3">
                <label class="text-sm text-white/70">Audio Source Language</label>
                <select id="audioFromLang" class="input mt-1">
                  <option value="auto">Auto Detect</option>
                  <option value="en">English</option>
                  <option value="hi">Hindi</option>
                  <option value="mr">Marathi</option>
                  <option value="gu">Gujarati</option>
                  <option value="ta">Tamil</option>
                  <option value="bn">Bengali</option>
                  <option value="te">Telugu</option>
                  <option value="kn">Kannada</option>
                </select>
              </div>
              <div class="mt-3">
                <label class="text-sm text-white/70">Audio Target Language</label>
                <select id="audioToLang" class="input mt-1">
                  <option value="hi">Hindi</option>
                  <option value="mr">Marathi</option>
                  <option value="gu">Gujarati</option>
                  <option value="ta">Tamil</option>
                  <option value="bn">Bengali</option>
                  <option value="en">English</option>
                  <option value="te">Telugu</option>
                  <option value="kn">Kannada</option>
                </select>
              </div>
              <div class="mt-3 flex gap-2">
                <button class="btn-primary" onclick="transcribeAudio()">Transcribe Audio</button>
                <button class="btn" onclick="transcribeAudioWithSpeechAPI()">Use Speech API</button>
                <button class="btn" onclick="translateAudioFile()">Translate Audio</button>
                <button class="btn" onclick="downloadAudioTranscript()">Download Transcript</button>
              </div>
              <div class="text-xs text-white/60 mt-2">
                üéØ Advanced audio processing with direct file transcription and translation<br>
                <span class="text-yellow-400">üí° Tip:</span> Use "Transcribe Audio" for analysis or "Use Speech API" for real transcription
              </div>
            </div>
          </div>
          <div>
            <label class="text-sm text-white/70">Audio Visualization & Processing</label>
            <div id="audioVisualizer" class="audio-visualizer">
              <div class="text-white/60 text-sm">Audio waveform will appear here after upload</div>
            </div>
            <div class="mt-3 flex gap-2 justify-center">
              <button id="playPauseBtn" class="btn" onclick="toggleAudioPlayback()" disabled>‚ñ∂ Play</button>
              <button id="stopBtn" class="btn" onclick="stopAudioPlayback()" disabled>‚èπ Stop</button>
              <button id="restartBtn" class="btn" onclick="restartAudioPlayback()" disabled>‚èÆ Restart</button>
            </div>
            <label class="text-sm text-white/70 mt-4">Audio Analysis Results</label>
            <div class="glass rounded-xl p-3 min-h-[120px]">
              <div id="audioAnalysis" class="text-sm text-white/80">Upload an audio file to see analysis results</div>
            </div>
            <div class="mt-3">
              <label class="text-sm text-white/70">Transcription Results</label>
              <div id="transcriptionResults" class="glass rounded-xl p-3 min-h-[100px] max-h-[200px] overflow-y-auto">
                <div class="text-white/60 text-sm">Transcription will appear here after processing</div>
              </div>
            </div>
          </div>
        </div>
      </section>
    </div>

    <div id="document-panel" class="panel">
      <!-- Document Translation -->
      <section class="card">
        <h2 class="text-xl font-semibold mb-3">üìÑ Advanced Document Translation</h2>
        <div class="grid lg:grid-cols-2 gap-6">
          <div>
            <label class="text-sm text-white/70">Upload Document (PDF / DOCX / PPTX)</label>
            <input type="file" id="docFile" accept=".pdf,.docx,.pptx" class="mt-2" onchange="handleDocUpload(event)" />
            <div class="mt-3 text-xs text-white/60">‚ú® Advanced parsing with format preservation ‚Ä¢ PDF generation ‚Ä¢ Layout maintenance</div>
            <div class="mt-3 flex gap-2">
              <button class="btn-primary" onclick="translateDocument()">Translate Document</button>
              <button class="btn" onclick="downloadTranslatedPDF()">Download PDF</button>
              <button class="btn" onclick="downloadTxt('document_translation.txt', document.getElementById('docTranslated').innerText)">Download TXT</button>
            </div>
          </div>
          <div>
            <label class="text-sm text-white/70">Side_by_Side Preview</label>
            <div class="grid grid-cols-2 gap-2 mt-2">
              <div id="docOriginal" class="glass rounded-xl p-3 min-h-[140px] overflow-auto text-sm">Original preview</div>
              <div id="docTranslated" class="glass rounded-xl p-3 min-h-[140px] overflow-auto text-sm">Translated preview</div>
            </div>
          </div>
        </div>
      </section>
    </div>

    <div id="video-panel" class="panel">
      <!-- Video Translation -->
      <section class="card">
        <h2 class="text-xl font-semibold mb-3">üé¨ Advanced Video Content Translation</h2>
        <div class="grid lg:grid-cols-2 gap-6">
          <div>
            <label class="text-sm text-white/70">Upload video or paste link (YouTube/Vimeo)</label>
            <input type="file" id="videoFile" accept="video/*" class="mt-2" onchange="handleVideoUpload(event)" />
            <input id="videoLink" class="input mt-2" placeholder="Paste YouTube/Vimeo link (optional)" />
            <button class="btn mt-2" onclick="loadVideoFromLink()">Load from Link</button>
            <div class="mt-3 flex gap-2">
              <button class="btn-primary" onclick="startVideoTranscription()">Transcribe (play & capture)</button>
              <button class="btn" onclick="stopVideoTranscription()">Stop Transcription</button>
              <button class="btn" onclick="extractOCR()">OCR Frame</button>
            </div>
            <div class="text-xs text-white/60 mt-2">
              üéØ Advanced video processing with multi-language subtitle support and OCR integration
            </div>
          </div>
          <div>
            <label class="text-sm text-white/70">Preview & Subtitle Generator</label>
            <div class="video-container">
              <video id="previewVideo" controls class="w-full rounded-xl bg-black">Your browser does not support HTML5 video.</video>
            </div>
            <div class="mt-3">
              <label class="text-sm text-white/70">Subtitle Language</label>
              <select id="subtitleLang" class="input mt-1">
                <option value="en">English</option>
                <option value="hi">Hindi</option>
                <option value="mr">Marathi</option>
                <option value="gu">Gujarati</option>
                <option value="ta">Tamil</option>
                <option value="bn">Bengali</option>
                <option value="te">Telugu</option>
                <option value="kn">Kannada</option>
              </select>
            </div>
            <div class="mt-3 flex gap-2">
              <button class="btn" onclick="downloadSRT()">Download SRT</button>
              <button class="btn" onclick="downloadVTT()">Download VTT</button>
            </div>
            <div id="subtitleEditor" class="mt-3 text-sm text-white/80">Subtitle editor will appear here after transcription.</div>
          </div>
        </div>
      </section>
      
      <section class="card">
        <h2 class="text-xl font-semibold mb-3">üéØ Video Subtitles Generator</h2>
        <div class="text-sm text-white/80">
          ‚ú® Advanced subtitle generation with language options, OCR text extraction, and timestamp synchronization. 
          Export in multiple formats for professional use.
        </div>
      </section>
    </div>

    <footer id="contact" class="card">
      <div class="container">
        <h2 class="section-title text-xl font-semibold mb-3">Contact</h2>
        <div class="flex gap-3 flex-wrap">
          <span class="chip bg-accent-600 px-3 py-2 rounded-full text-sm">‚úâÔ∏è 202krishnapatil@gmail.com</span>
          <span class="chip bg-accent-600 px-3 py-2 rounded-full text-sm">üìû +91 95801 59631</span>
          <a class="chip bg-accent-600 px-3 py-2 rounded-full text-sm hover:bg-accent-500 transition-colors" target="_blank" rel="noopener" href="https://in.linkedin.com/in/krishna-chandrakant-patil-33969536b">in LinkedIn</a>
        </div>
        <p class="muted text-white/60 mt-4">¬© <span id="y"></span> Krishna Patil ‚Ä¢ Advanced AI Translation System</p>
      </div>
    </footer>
  </div>

<script>
  // Nav tab switching
  document.querySelectorAll('.nav-btn').forEach(btn => {
    btn.addEventListener('click', function() {
      document.querySelectorAll('.nav-btn').forEach(b => b.classList.remove('nav-active'));
      this.classList.add('nav-active');
      document.querySelectorAll('.panel').forEach(p => p.classList.remove('active'));
      const panelId = this.getAttribute('data-panel');
      const panel = document.getElementById(panelId);
      if (panel) panel.classList.add('active');
    });
  });

// --- Helpers & language locale map ---
const LANG_TO_LOCALE = {
  en: 'en-US', hi: 'hi-IN', mr: 'mr-IN', gu: 'gu-IN', ta: 'ta-IN',
  bn: 'bn-IN', te: 'te-IN', kn: 'kn-IN', es: 'es-ES', fr: 'fr-FR',
  de: 'de-DE', ja: 'ja-JP'
};

function langToLocale(code){
  if(!code) return 'en-US';
  if(LANG_TO_LOCALE[code]) return LANG_TO_LOCALE[code];
  // if user passed e.g. "en-US" already, return it
  if(code.includes('-')) return code;
  return code + '-IN'; // fallback (for many Indian langs)
}

function $id(id){ return document.getElementById(id); }

function parseGoogleTranslate(data){
  // translate_a/single returns nested arrays: data[0] => segments: [[translated, original, ...], ...]
  try{
    if(Array.isArray(data) && Array.isArray(data[0])){
      return data[0].map(seg => (Array.isArray(seg) ? (seg[0]||'') : '')).join('');
    }
    // fallback: maybe responseData style
    if(data && data.responseData && data.responseData.translatedText) return data.responseData.translatedText;
    return '';
  }catch(e){
    console.warn('parseGoogleTranslate error', e);
    return '';
  }
}

function escapeRegex(s){ return s.replace(/[.*+?^${}()|[\]\\]/g,'\\$&'); }

function applyReplacements(text, replacements){
  if(!replacements || typeof replacements !== 'object') return text;
  let out = text;
  const keys = Object.keys(replacements).sort((a,b)=> b.length - a.length);
  for(const k of keys){
    if(!k) continue;
    const v = replacements[k];
    const isLatin = /[A-Za-z]/.test(k);
    if(isLatin){
      const re = new RegExp('\\b' + escapeRegex(k) + '\\b', 'ig');
      out = out.replace(re, (m) => {
        // preserve capitalization
        if(m[0] === m[0].toUpperCase()) return v.charAt(0).toUpperCase() + v.slice(1);
        return v;
      });
    } else {
      const re = new RegExp(escapeRegex(k), 'g');
      out = out.replace(re, v);
    }
  }
  return out;
}

// small rule maps for rewriteStyle (expand these for your demo)
const RULES = {
  en: {
    formal: {
      "hi":"hello", "hey":"hello", "thanks":"thank you", "gonna":"going to", "wanna":"want to", "ok":"okay"
    },
    informal: {
      "hello":"hey", "please inform me":"let me know", "please":"pls", "do not":"don't"
    }
  },
  hi: {
    formal: { "‡§π‡§æ‡§Ø":"‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞", "‡§π‡•á‡§≤‡•ã":"‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞", "‡§¨‡§§‡§æ‡§ì":"‡§ï‡•É‡§™‡§Ø‡§æ ‡§¨‡§§‡§æ‡§á‡§è" },
    informal: { "‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞":"‡§π‡§æ‡§Ø", "‡§ï‡•É‡§™‡§Ø‡§æ":"‡§™‡•ç‡§≤‡•Ä‡§ú" }
  },
  default: { formal: {}, informal: {} }
};

// simple grammar heuristics per language
function grammarFixSimple(text, lang){
  if(!text) return text;
  lang = (lang||'en').toLowerCase();
  // English: punctuation/spacing, sentence capitalization
  if(lang === 'en'){
    let t = text.replace(/\s+/g,' ').trim();
    t = t.replace(/\s+([,?.!;:])/g,'$1'); // fix spaces before punctuation
    t = t.replace(/([.!?])\s*([a-z])/g, (m,p,c)=> p + ' ' + c.toUpperCase()); // capitalize after sentence end
    t = t.replace(/^([a-z])/,(m,c)=> c.toUpperCase()); // capitalize first char
    t = t.replace(/(\s)+/g,' ');
    return t;
  }
  // Devanagari / Indian languages: normalize spaces & punctuation
  if(['hi','mr','gu','bn','te','kn','ta'].includes(lang)){
    let t = text.replace(/\s+/g,' ').trim();
    t = t.replace(/\s+([,‡•§!?])/g,'$1');
    return t;
  }
  // fallback generic normalization
  return text.replace(/\s+/g,' ').trim();
}

/* -------------------- Translation function -------------------- */
/* Returns the translated string (and sets outputText). Uses google translate public endpoint */
async function translateText(){
  const input = $id('inputText').value.trim();
  const fromLang = $id('fromLang').value || 'auto';
  const toLang = $id('toLang').value || 'en';
  if(!input){ alert('Please enter some text.'); return ''; }

  const domain = $id('domain') ? $id('domain').value : 'General';
  const contextual = (domain && domain !== 'General') ? `[Context:${domain}] ${input}` : input;

  const url = `https://translate.googleapis.com/translate_a/single?client=gtx&sl=${encodeURIComponent(fromLang)}&tl=${encodeURIComponent(toLang)}&dt=t&q=${encodeURIComponent(contextual)}`;

  try{
    const res = await fetch(url);
    if(!res.ok) throw new Error('HTTP '+res.status);
    const data = await res.json();
    const translated = parseGoogleTranslate(data) || '';
    $id('outputText').value = translated;
    return translated;
  }catch(err){
    console.error('Translate error', err);
    alert('Translation failed: ' + (err.message || err));
    return '';
  }
}

/* -------------------- Grammar Fix -------------------- */
/* This now: 1) gets the translated text (in target language) and 2) runs grammar heuristics */
async function grammarFix(){
  // get a translated base (ensures we're fixing the selected output language)
  const translated = await translateText();
  if(!translated) return;
  const toLang = $id('toLang').value || 'en';
  const fixed = grammarFixSimple(translated, toLang);
  $id('outputText').value = fixed;
}

/* -------------------- Rewrite (formal/informal) -------------------- */
async function rewriteStyle(style){
  // style: 'formal' or 'informal'
  const translated = await translateText();
  if(!translated) return;
  const toLang = $id('toLang').value || 'en';
  const rules = RULES[toLang] || RULES.default;
  let out = translated;
  if(style === 'formal' && rules.formal) out = applyReplacements(out, rules.formal);
  else if(style === 'informal' && rules.informal) out = applyReplacements(out, rules.informal);
  // apply a grammar normalization afterwards
  out = grammarFixSimple(out, toLang);
  $id('outputText').value = out;
}

/* -------------------- Enhanced Speech Recognition / Live Translate -------------------- */
let recognition = null;

function startListening(){
  // Enhanced browser compatibility check
  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
  
  if(!SpeechRecognition){ 
    // Provide alternative solutions for unsupported browsers
    const alternatives = [
      'Speech Recognition is not supported in this browser.',
      'Please try:',
      '‚Ä¢ Chrome/Edge (recommended)',
      '‚Ä¢ Firefox with speech recognition extension',
      '‚Ä¢ Use text input instead',
      '‚Ä¢ Check microphone permissions'
    ].join('\n');
    
    alert(alternatives);
    return; 
  }
  
  if(recognition){ recognition.stop(); recognition = null; }
  
  try {
    recognition = new SpeechRecognition();
    const from = $id('fromLang').value || 'en';
    recognition.lang = langToLocale(from);
    recognition.interimResults = true;
    recognition.continuous = true;

    recognition.onresult = (event) => {
      let transcript = '';
      for(let i = event.resultIndex; i < event.results.length; ++i){
        transcript += event.results[i][0].transcript;
      }
      // show interim transcripts in input box (non-final)
      $id('inputText').value = transcript;
      
      // Update UI to show recognition is active
      const micBtn = document.getElementById('micBtn');
      if (micBtn) {
        micBtn.innerHTML = 'üéôÔ∏è Listening...';
        micBtn.classList.add('btn-primary');
        micBtn.classList.remove('btn');
      }
    };
    
    recognition.onerror = (e) => {
      console.error('rec error', e);
      let errorMessage = 'Speech recognition error: ';
      
      switch(e.error) {
        case 'not-allowed':
          errorMessage += 'Microphone access denied. Please allow microphone access.';
          break;
        case 'no-speech':
          errorMessage += 'No speech detected. Please speak clearly.';
          break;
        case 'audio-capture':
          errorMessage += 'Audio capture failed. Check your microphone.';
          break;
        case 'network':
          errorMessage += 'Network error. Check your internet connection.';
          break;
        default:
          errorMessage += e.error;
      }
      
      alert(errorMessage);
      resetMicButton();
    };
    
    recognition.onend = () => {
      console.log('recognition ended');
      resetMicButton();
    };
    
    recognition.start();
    
  } catch (err) {
    console.error('Speech recognition setup failed:', err);
    alert('Speech recognition setup failed: ' + err.message);
  }
}

function resetMicButton() {
  const micBtn = document.getElementById('micBtn');
  if (micBtn) {
    micBtn.innerHTML = 'üéô Start Mic';
    micBtn.classList.remove('btn-primary');
    micBtn.classList.add('btn');
  }
}
function stopListening(){ if(recognition){ recognition.stop(); recognition=null; } }

let liveRec = null;
function startTranslateLive(){
  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
  if(!SpeechRecognition){ alert('Speech Recognition not supported'); return; }
  if(liveRec){ liveRec.stop(); liveRec = null; }
  
  liveRec = new SpeechRecognition(); 
  liveRec.continuous = true; 
  liveRec.interimResults = true; 
  
  // Use the live speech language selection
  const from = document.getElementById('liveSpeechLang').value || 'auto';
  liveRec.lang = langToLocale(from);

  liveRec.onresult = async (e) => {
    let finalTranscript = '';
    let interimTranscript = '';
    
    for(let i=0;i<e.results.length;i++){
      const res = e.results[i];
      if(res.isFinal){ 
        finalTranscript += res[0].transcript + ' '; 
      } else {
        interimTranscript += res[0].transcript;
      }
    }
    
    // Display both interim and final results
    const liveTranscriptDiv = document.getElementById('liveTranscript');
    if (liveTranscriptDiv) {
      liveTranscriptDiv.innerHTML = `
        <div class="text-white/60 text-sm mb-2">Interim: ${interimTranscript}</div>
        <div class="text-white text-sm">Final: ${finalTranscript}</div>
      `;
    }
    
    if(finalTranscript.trim()){
      // Update main input text
      const inputText = document.getElementById('inputText');
      if (inputText) inputText.value = finalTranscript;
      
      // Auto-translate if target language is set
      try{ 
        await translateText(); 
      } catch(ex){ 
        console.error('Live translation error:', ex); 
      }
    }
  };
  
  liveRec.onerror = (ev)=> {
    console.error('liveRec err', ev);
    let errorMessage = 'Speech recognition error: ';
    
    switch(ev.error) {
      case 'not-allowed':
        errorMessage += 'Microphone access denied. Please allow microphone access.';
        break;
      case 'no-speech':
        errorMessage += 'No speech detected. Please speak clearly.';
        break;
      case 'audio-capture':
        errorMessage += 'Audio capture failed. Check your microphone.';
        break;
      case 'network':
        errorMessage += 'Network error. Check your internet connection.';
        break;
      default:
        errorMessage += ev.error;
    }
    
    alert(errorMessage);
  };
  
  liveRec.onend = () => {
    console.log('Live speech recognition ended');
  };
  
  liveRec.start();
}
function stopTranslateLive(){ if(liveRec){ liveRec.stop(); liveRec=null; } }

/* -------------------- Speak output -------------------- */
function speakText(){
  const text = $id('outputText').value;
  const to = $id('toLang').value || 'en';
  if(!text) return alert('Nothing to speak');
  const u = new SpeechSynthesisUtterance(text);
  u.lang = langToLocale(to);
  speechSynthesis.speak(u);
}

/* -------------------- Small utilities (copy/download) -------------------- */
function clearInput(){ $id('inputText').value=''; }
function copyOut(){ navigator.clipboard.writeText($id('outputText').value||''); }
function downloadTxt(name, text){
  const a = document.createElement('a');
  const blob = new Blob([text||''], {type:'text/plain'});
  a.href = URL.createObjectURL(blob);
  a.download = name;
  a.click();
  URL.revokeObjectURL(a.href);
}

// (optional) export some functions to window if your HTML uses inline onclick="..."
window.translateText = translateText;
window.grammarFix = grammarFix;
window.rewriteStyle = rewriteStyle;
window.startListening = startListening;
window.stopListening = stopListening;
window.startTranslateLive = startTranslateLive;
window.stopTranslateLive = stopTranslateLive;
window.speakText = speakText;
window.clearInput = clearInput;
window.copyOut = copyOut;
window.downloadTxt = downloadTxt;

// Add missing functions
function setTone(tone) {
  // This function can be expanded to apply tone-specific translations
  console.log('Tone set to:', tone);
  // For now, just log the tone selection
}

// Export all functions to window for inline onclick handlers
window.setTone = setTone;
window.handleDocUpload = handleDocUpload;
window.translateDocument = translateDocument;
window.downloadTranslatedPDF = downloadTranslatedPDF;
window.handleAudioUpload = handleAudioUpload;
window.transcribeAudio = transcribeAudio;
window.transcribeAudioWithSpeechAPI = transcribeAudioWithSpeechAPI;
window.translateAudioFile = translateAudioFile;
window.downloadAudioTranscript = downloadAudioTranscript;
window.toggleAudioPlayback = toggleAudioPlayback;
window.stopAudioPlayback = stopAudioPlayback;
window.restartAudioPlayback = restartAudioPlayback;
window.handleVideoUpload = handleVideoUpload;
window.startVideoTranscription = startVideoTranscription;
window.stopVideoTranscription = stopVideoTranscription;
window.extractOCR = extractOCR;
window.renderSubtitleEditor = renderSubtitleEditor;
window.downloadSRT = downloadSRT;
window.downloadVTT = downloadVTT;
window.toSrtTime = toSrtTime;
window.toggleChat = toggleChat;
window.translateChat = translateChat;
window.clearChat = clearChat;
window.downloadChat = downloadChat;
window.exportChat = exportChat;

// Function to handle video links (YouTube/Vimeo)
function loadVideoFromLink() {
  const link = document.getElementById('videoLink').value.trim();
  if (!link) {
    alert('Please enter a video link');
    return;
  }
  
  // For now, show a message about link handling
  // In a production environment, you would need to implement proper video extraction
  const subtitleEditor = document.getElementById('subtitleEditor');
  subtitleEditor.innerHTML = '<div class="text-yellow-400 text-sm">‚ö†Ô∏è Video link processing requires server-side implementation. For now, please upload video files directly.</div>';
  
  // Clear the link input
  document.getElementById('videoLink').value = '';
}

window.loadVideoFromLink = loadVideoFromLink;

// Chat translator functionality
let chatMessages = [];

function toggleChat() {
  const chatInterface = document.getElementById('chatInterface');
  chatInterface.classList.toggle('active');
}

async function translateChat() {
  const input = document.getElementById('chatInput').value.trim();
  if (!input) {
    alert('Please enter a message to translate');
    return;
  }
  
  const fromLang = document.getElementById('chatFromLang').value || 'auto';
  const toLang = document.getElementById('chatToLang').value || 'en';
  
  try {
    // Translate the message
    const url = `https://translate.googleapis.com/translate_a/single?client=gtx&sl=${encodeURIComponent(fromLang)}&tl=${encodeURIComponent(toLang)}&dt=t&q=${encodeURIComponent(input)}`;
    const res = await fetch(url);
    if (!res.ok) throw new Error('HTTP ' + res.status);
    
    const data = await res.json();
    const translated = parseGoogleTranslate(data) || '';
    
    // Add message to chat history
    const message = {
      id: Date.now(),
      original: input,
      translated: translated,
      fromLang: fromLang,
      toLang: toLang,
      timestamp: new Date().toLocaleTimeString()
    };
    
    chatMessages.push(message);
    renderChatHistory();
    
    // Clear input
    document.getElementById('chatInput').value = '';
    
  } catch (err) {
    console.error('Chat translation failed:', err);
    alert('Translation failed: ' + err.message);
  }
}

function renderChatHistory() {
  const history = document.getElementById('chatHistory');
  if (chatMessages.length === 0) {
    history.innerHTML = '<div class="text-white/60 text-sm">Chat messages will appear here...</div>';
    return;
  }
  
  history.innerHTML = chatMessages.map(msg => `
    <div class="mb-3 p-2 bg-white/5 rounded">
      <div class="text-xs text-white/60 mb-1">${msg.timestamp} (${msg.fromLang} ‚Üí ${msg.toLang})</div>
      <div class="text-white/80 mb-1"><strong>Original:</strong> ${msg.original}</div>
      <div class="text-white"><strong>Translated:</strong> ${msg.translated}</div>
    </div>
  `).join('');
  
  // Scroll to bottom
  history.scrollTop = history.scrollHeight;
}

function clearChat() {
  chatMessages = [];
  renderChatHistory();
}

function downloadChat() {
  if (chatMessages.length === 0) {
    alert('No chat messages to download');
    return;
  }
  
  const chatText = chatMessages.map(msg => 
    `[${msg.timestamp}] ${msg.fromLang} ‚Üí ${msg.toLang}\nOriginal: ${msg.original}\nTranslated: ${msg.translated}\n`
  ).join('\n');
  
  downloadTxt('chat_translation.txt', chatText);
}

function exportChat() {
  if (chatMessages.length === 0) {
    alert('No chat messages to export');
    return;
  }
  
  const chatJson = JSON.stringify(chatMessages, null, 2);
  const blob = new Blob([chatJson], { type: 'application/json' });
  const url = URL.createObjectURL(blob);
  const a = document.createElement('a');
  a.href = url;
  a.download = 'chat_translation.json';
  a.click();
  URL.revokeObjectURL(url);
}

// Export chat functions to window
window.translateChat = translateChat;
window.clearChat = clearChat;
window.downloadChat = downloadChat;
window.exportChat = exportChat;

// Initialize the year in footer and add comprehensive error handling
document.addEventListener('DOMContentLoaded', function() {
  document.getElementById('y').textContent = new Date().getFullYear();
  
  // Check browser compatibility
  checkBrowserCompatibility();
  
  // Add user guidance
  addUserGuidance();
});

function checkBrowserCompatibility() {
  const issues = [];
  
  // Check Speech Recognition
  if (!window.SpeechRecognition && !window.webkitSpeechRecognition) {
    issues.push('Speech Recognition not supported - Voice features limited');
  }
  
  // Check File API
  if (!window.FileReader) {
    issues.push('File API not supported - Document upload limited');
  }
  
  // Check Web Audio API
  if (!window.AudioContext && !window.webkitAudioContext) {
    issues.push('Web Audio API not supported - Audio processing limited');
  }
  
  if (issues.length > 0) {
    console.warn('Browser compatibility issues:', issues);
    
    // Show user-friendly warning
    const warningDiv = document.createElement('div');
    warningDiv.className = 'message warning';
    warningDiv.innerHTML = `
      <div class="flex items-center space-x-2">
        <span>‚ö†Ô∏è</span>
        <span><strong>Browser Compatibility Notice:</strong> Some features may be limited in your browser.</span>
      </div>
      <div class="text-xs mt-2">For best experience, use Chrome, Edge, or Firefox with latest updates.</div>
    `;
    
    // Insert after header
    const header = document.querySelector('header');
    if (header) {
      header.parentNode.insertBefore(warningDiv, header.nextSibling);
    }
  }
}

function addUserGuidance() {
  // Add helpful tooltips and guidance
  const tooltips = [
    { selector: '#fromLang', text: 'Select source language or use Auto Detect' },
    { selector: '#toLang', text: 'Choose target language for translation' },
    { selector: '#domain', text: 'Select context for better translation accuracy' },
    { selector: '#micBtn', text: 'Click to start voice input (requires microphone permission)' },
    { selector: '.chat-toggle', text: 'Access real-time chat translator' }
  ];
  
  tooltips.forEach(tooltip => {
    const element = document.querySelector(tooltip.selector);
    if (element) {
      element.title = tooltip.text;
    }
  });
}

  // --- Document parsing ---
  let lastDocText = '';
  async function handleDocUpload(e){
    const f = e.target.files[0]; if(!f) return;
    const orig = document.getElementById('docOriginal'); const translated = document.getElementById('docTranslated');
    orig.innerHTML = 'Loading...'; translated.innerHTML = '';
    const name = f.name.toLowerCase();
    
    try {
      if(name.endsWith('.docx')){
        const arrayBuffer = await f.arrayBuffer();
        const res = await mammoth.convertToHtml({arrayBuffer});
        orig.innerHTML = res.value;
        lastDocText = (res.value || '').replace(/<[^>]*>/g,' ');
      }else if(name.endsWith('.pdf')){
        const arrayBuffer = await f.arrayBuffer();
        const uint8 = new Uint8Array(arrayBuffer);
        const loadingTask = pdfjsLib.getDocument({data:uint8});
        const pdf = await loadingTask.promise;
        let full='';
        for(let i=1;i<=pdf.numPages;i++){ 
          const page = await pdf.getPage(i); 
          const content = await page.getTextContent(); 
          const strs = content.items.map(it=> it.str).join(' '); 
          full += ' ' + strs; 
        }
        orig.innerText = full.slice(0, 20000);
        lastDocText = full;
      }else if(name.endsWith('.pptx')){
        const arrayBuffer = await f.arrayBuffer();
        const zip = await JSZip.loadAsync(arrayBuffer);
        const slides = Object.keys(zip.files).filter(p=> p.startsWith('ppt/slides/slide'));
        let text='';
        for(const s of slides){ 
          const content = await zip.files[s].async('string');
          const t = (content.match(/<a:t>(.*?)<\/a:t>/g)||[]).map(x=> x.replace(/<.*?>/g,'')).join(' ');
          text += ' ' + t;
        }
        orig.innerText = text.slice(0,20000);
        lastDocText = text;
      }else{
        orig.innerText = 'Unsupported file format';
      }
    } catch(err) {
      orig.innerText = 'File parsing failed: ' + err.message;
      console.error('Document parsing error:', err);
    }
  }

  // --- Enhanced Document Translation with PDF Generation ---
  async function translateDocument(){
    if(!lastDocText){ alert('No document text loaded (upload a file first)'); return; }
    
    const translatedDiv = document.getElementById('docTranslated');
    translatedDiv.innerHTML = '<div class="text-yellow-400">üîÑ Translating document... Please wait.</div>';
    
    const tgt = document.getElementById('toLang').value || 'en';
    const from = document.getElementById('fromLang').value || 'auto';
    const chunks = chunkString(lastDocText, 4000);
    let translated = '';
    
    try {
      for(let i = 0; i < chunks.length; i++){
        const c = chunks[i];
        const url = `https://translate.googleapis.com/translate_a/single?client=gtx&sl=${from}&tl=${tgt}&dt=t&q=${encodeURIComponent(c)}`;
        
        const r = await fetch(url); 
        const d = await r.json(); 
        
        if(Array.isArray(d) && d[0] && Array.isArray(d[0])){
          d[0].forEach(segment => {
            if(Array.isArray(segment) && segment[0]){
              translated += segment[0];
            }
          });
        }
        
        // Update progress
        const progress = Math.round(((i + 1) / chunks.length) * 100);
        translatedDiv.innerHTML = `<div class="text-blue-400">üîÑ Translating... ${progress}% complete</div>`;
      }
      
      // Display translated text
      translatedDiv.innerHTML = `
        <div class="text-green-400 mb-2">‚úÖ Translation complete!</div>
        <div class="text-white/80">${translated}</div>
      `;
      
      // Store translated text for PDF generation
      window.translatedDocumentText = translated;
      window.originalDocumentText = lastDocText;
      
    } catch(e){ 
      console.error('Document translation failed:', e); 
      translatedDiv.innerHTML = '<div class="text-red-400">‚ùå Translation failed: ' + e.message + '</div>';
    }
  }
  
  async function downloadTranslatedPDF() {
    if (!window.translatedDocumentText) {
      alert('Please translate a document first');
      return;
    }
    
    try {
      const { jsPDF } = window.jspdf;
      const doc = new jsPDF();
      
      // Set font for better language support
      doc.setFont('helvetica');
      doc.setFontSize(12);
      
      // Add title
      doc.setFontSize(16);
      doc.text('Translated Document', 20, 20);
      doc.setFontSize(12);
      
      // Add original and translated text
      const margin = 20;
      const lineHeight = 7;
      let y = 40;
      
      // Original text (first page)
      doc.setFontSize(14);
      doc.text('Original Text:', margin, y);
      y += lineHeight;
      doc.setFontSize(10);
      
      const originalLines = doc.splitTextToSize(window.originalDocumentText.substring(0, 1000), 170);
      doc.text(originalLines, margin, y);
      y += originalLines.length * lineHeight + 10;
      
      // Translated text (continue on same page if space, otherwise new page)
      if (y > 250) {
        doc.addPage();
        y = 20;
      }
      
      doc.setFontSize(14);
      doc.text('Translated Text:', margin, y);
      y += lineHeight;
      doc.setFontSize(10);
      
      const translatedLines = doc.splitTextToSize(window.translatedDocumentText, 170);
      doc.text(translatedLines, margin, y);
      
      // Save the PDF
      doc.save('translated_document.pdf');
      
    } catch (err) {
      console.error('PDF generation failed:', err);
      alert('PDF generation failed: ' + err.message);
    }
  }

  function chunkString(str, size){ const chunks=[]; for(let i=0;i<str.length;i+=size) chunks.push(str.slice(i,i+size)); return chunks; }

  let audioFile = null;
  let wavesurfer = null;
  let audioTranscript = '';
  let audioSegments = [];
  let audioElement = null;
  let isPlaying = false;
  
  function handleAudioUpload(e){ 
    const f = e.target.files[0]; 
    if(!f) return; 
    
    audioFile = f;
    
    // Initialize waveform visualization
    if (wavesurfer) {
      wavesurfer.destroy();
    }
    
    try {
      wavesurfer = WaveSurfer.create({
        container: '#audioVisualizer',
        waveColor: '#8b5cf6',
        progressColor: '#6ef3ff',
        cursorColor: '#ffffff',
        barWidth: 2,
        barRadius: 3,
        cursorWidth: 1,
        height: 80,
        barGap: 3,
        interact: true,
        hideScrollbar: true
      });
      
      // Load audio file
      const url = URL.createObjectURL(f);
      wavesurfer.load(url);
      
      // Create audio element for playback
      audioElement = new Audio(url);
      
      // Enable playback controls
      document.getElementById('playPauseBtn').disabled = false;
      document.getElementById('stopBtn').disabled = false;
      document.getElementById('restartBtn').disabled = false;
      
      // Set up waveform events
      wavesurfer.on('ready', function() {
        console.log('Waveform ready');
      });
      
      wavesurfer.on('audioprocess', function(currentTime) {
        // Update audio element time to sync with waveform
        if (audioElement && !isPlaying) {
          audioElement.currentTime = currentTime;
        }
      });
      
      // Show success message
      document.getElementById('audioAnalysis').innerHTML = '<div class="text-green-400 text-sm">‚úÖ Audio loaded successfully! Use "Transcribe Audio" to process.</div>';
      
      // Clear previous results
      document.getElementById('transcriptionResults').innerHTML = '<div class="text-white/60 text-sm">Transcription will appear here after processing</div>';
      
    } catch (err) {
      console.error('Waveform creation failed:', err);
      document.getElementById('audioAnalysis').innerHTML = '<div class="text-red-400 text-sm">‚ùå Waveform creation failed. Audio file loaded but visualization unavailable.</div>';
    }
  }
  
  async function transcribeAudio() {
    if (!audioFile) {
      alert('Please upload an audio file first');
      return;
    }
    
    try {
      // Show processing status
      document.getElementById('audioAnalysis').innerHTML = '<div class="text-yellow-400 text-sm">üîÑ Processing audio file... Please wait.</div>';
      
      // Create audio context for processing
      const audioContext = new (window.AudioContext || window.webkitAudioContext)();
      const arrayBuffer = await audioFile.arrayBuffer();
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
      
      // Extract audio data for analysis
      const channelData = audioBuffer.getChannelData(0);
      const sampleRate = audioBuffer.sampleRate;
      const duration = audioBuffer.duration;
      
      // Advanced audio analysis with speech detection
      const segments = analyzeAudioSegments(channelData, sampleRate, duration);
      audioSegments = segments;
      
      // Generate simulated transcription based on audio analysis
      audioTranscript = generateSimulatedTranscription(segments, duration);
      
      // Display results
      displayAudioAnalysis(segments, duration);
      displayTranscriptionResults(audioTranscript);
      
      // Show success message
      document.getElementById('audioAnalysis').innerHTML = `
        <div class="text-green-400 text-sm mb-2">‚úÖ Audio analysis complete!</div>
        <div class="text-white/80 text-sm">
          Duration: ${duration.toFixed(2)}s<br>
          Segments detected: ${segments.length}<br>
          Speech detected: ${segments.filter(s => s.hasSpeech).length} segments
        </div>
      `;
      
    } catch (err) {
      console.error('Audio processing failed:', err);
      document.getElementById('audioAnalysis').innerHTML = '<div class="text-red-400 text-sm">‚ùå Audio processing failed: ' + err.message + '</div>';
    }
  }
  
  function analyzeAudioSegments(channelData, sampleRate, duration) {
    // Advanced segment detection with improved speech analysis
    const segments = [];
    const segmentLength = 2; // 2 second segments for better accuracy
    const samplesPerSegment = segmentLength * sampleRate;
    
    for (let i = 0; i < channelData.length; i += samplesPerSegment) {
      const segmentData = channelData.slice(i, i + samplesPerSegment);
      const amplitude = Math.sqrt(segmentData.reduce((sum, sample) => sum + sample * sample, 0) / segmentData.length);
      
      // Enhanced speech detection with multiple criteria
      const hasSpeech = detectSpeechSegment(segmentData, sampleRate, amplitude);
      const speechConfidence = calculateSpeechConfidence(segmentData, sampleRate, amplitude);
      
      segments.push({
        start: i / sampleRate,
        end: Math.min((i + samplesPerSegment) / sampleRate, duration),
        amplitude: amplitude,
        hasSpeech: hasSpeech,
        speechConfidence: speechConfidence,
        energy: calculateEnergy(segmentData),
        frequency: analyzeFrequencyContent(segmentData, sampleRate)
      });
    }
    
    return segments;
  }
  
  function detectSpeechSegment(channelData, sampleRate, amplitude) {
    // Multi-criteria speech detection
    const energy = calculateEnergy(channelData);
    const zeroCrossings = calculateZeroCrossings(channelData);
    const spectralCentroid = calculateSpectralCentroid(channelData, sampleRate);
    
    // Speech typically has:
    // - Moderate to high amplitude (not too quiet, not too loud)
    // - Moderate energy levels
    // - Reasonable zero-crossing rate (indicates frequency content)
    // - Spectral centroid in human speech range (85-255 Hz for fundamental, 2-8 kHz for formants)
    
    const amplitudeOK = amplitude > 0.003 && amplitude < 0.8;
    const energyOK = energy > 0.0001 && energy < 0.5;
    const zeroCrossingsOK = zeroCrossings > 100 && zeroCrossings < 5000;
    const spectralOK = spectralCentroid > 500 && spectralCentroid < 8000;
    
    return amplitudeOK && energyOK && zeroCrossingsOK && spectralOK;
  }
  
  function calculateSpeechConfidence(channelData, sampleRate, amplitude) {
    // Calculate confidence score for speech detection
    const energy = calculateEnergy(channelData);
    const zeroCrossings = calculateZeroCrossings(channelData);
    const spectralCentroid = calculateSpectralCentroid(channelData, sampleRate);
    
    let confidence = 0;
    
    // Amplitude confidence (0-25 points)
    if (amplitude > 0.01 && amplitude < 0.6) confidence += 25;
    else if (amplitude > 0.005 && amplitude < 0.8) confidence += 15;
    
    // Energy confidence (0-25 points)
    if (energy > 0.001 && energy < 0.3) confidence += 25;
    else if (energy > 0.0001 && energy < 0.5) confidence += 15;
    
    // Zero-crossing confidence (0-25 points)
    if (zeroCrossings > 200 && zeroCrossings < 3000) confidence += 25;
    else if (zeroCrossings > 100 && zeroCrossings < 5000) confidence += 15;
    
    // Spectral confidence (0-25 points)
    if (spectralCentroid > 1000 && spectralCentroid < 6000) confidence += 25;
    else if (spectralCentroid > 500 && spectralCentroid < 8000) confidence += 15;
    
    return Math.min(confidence, 100);
  }
  
  function calculateZeroCrossings(channelData) {
    let crossings = 0;
    for (let i = 1; i < channelData.length; i++) {
      if ((channelData[i] >= 0 && channelData[i-1] < 0) || 
          (channelData[i] < 0 && channelData[i-1] >= 0)) {
        crossings++;
      }
    }
    return crossings;
  }
  
  function calculateSpectralCentroid(channelData, sampleRate) {
    // Simplified spectral centroid calculation
    // In production, you'd use FFT for accurate frequency analysis
    const energy = calculateEnergy(channelData);
    const zeroCrossings = calculateZeroCrossings(channelData);
    
    // Estimate frequency based on zero-crossing rate
    // Frequency ‚âà (zeroCrossings * sampleRate) / (2 * channelData.length)
    const estimatedFreq = (zeroCrossings * sampleRate) / (2 * channelData.length);
    
    return estimatedFreq;
  }
  
  function analyzeFrequencyContent(channelData, sampleRate) {
    // Basic frequency analysis
    const zeroCrossings = calculateZeroCrossings(channelData);
    const estimatedFreq = (zeroCrossings * sampleRate) / (2 * channelData.length);
    
    return {
      dominant: estimatedFreq,
      range: {
        low: Math.max(0, estimatedFreq * 0.5),
        high: Math.min(sampleRate / 2, estimatedFreq * 2)
      }
    };
  }
  
  function calculateEnergy(channelData) {
    return channelData.reduce((sum, sample) => sum + sample * sample, 0) / channelData.length;
  }
  
  function generateSimulatedTranscription(segments, duration) {
    // Generate realistic transcription based on enhanced audio analysis
    let transcript = '';
    let speechSegments = segments.filter(s => s.hasSpeech);
    let totalSpeechTime = speechSegments.reduce((sum, s) => sum + (s.end - s.start), 0);
    
    if (speechSegments.length === 0) {
      transcript = `[00:00 - ${formatTime(duration)}] Audio content detected but no clear speech patterns identified.\n\n`;
      return transcript;
    }
    
    // Generate more realistic transcription based on speech characteristics
    speechSegments.forEach((segment, index) => {
      const segmentDuration = segment.end - segment.start;
      const confidence = segment.speechConfidence;
      const frequency = segment.frequency;
      
      // Estimate words based on duration and confidence
      const wordsPerSecond = Math.max(1, Math.min(3, confidence / 25)); // 1-3 words per second based on confidence
      const estimatedWords = Math.floor(segmentDuration * wordsPerSecond);
      
      // Generate context-aware placeholder text
      const placeholderText = generateContextualPlaceholderText(estimatedWords, segment, index, speechSegments.length);
      
      transcript += `[${formatTime(segment.start)} - ${formatTime(segment.end)}] ${placeholderText}\n`;
      transcript += `  Confidence: ${confidence}% | Frequency: ${Math.round(frequency.dominant)}Hz\n\n`;
    });
    
    // Add summary
    transcript += `\n--- TRANSCRIPTION SUMMARY ---\n`;
    transcript += `Total Duration: ${formatTime(duration)}\n`;
    transcript += `Speech Segments: ${speechSegments.length}\n`;
    transcript += `Speech Time: ${formatTime(totalSpeechTime)}\n`;
    transcript += `Average Confidence: ${Math.round(speechSegments.reduce((sum, s) => sum + s.speechConfidence, 0) / speechSegments.length)}%\n`;
    
    return transcript;
  }
  
  function generateContextualPlaceholderText(wordCount, segment, index, totalSegments) {
    // Generate context-aware placeholder text based on segment characteristics
    const templates = {
      high: [
        "Clear speech detected with high confidence. This segment contains natural language patterns typical of human conversation.",
        "Strong speech signal identified. The audio shows clear pronunciation and natural speech rhythm.",
        "High-quality speech segment detected. Language patterns indicate clear communication."
      ],
      medium: [
        "Moderate speech clarity detected. Some background noise may be present but speech is distinguishable.",
        "Speech patterns identified with medium confidence. Content appears to be conversational.",
        "Clear speech with moderate background interference. Language patterns are recognizable."
      ],
      low: [
        "Speech detected with lower confidence. Background noise may affect clarity.",
        "Faint speech patterns identified. Some words may be difficult to distinguish.",
        "Speech present but with reduced clarity. Content may require enhanced processing."
      ]
    };
    
    // Determine confidence level
    let confidenceLevel = 'medium';
    if (segment.speechConfidence >= 75) confidenceLevel = 'high';
    else if (segment.speechConfidence <= 50) confidenceLevel = 'low';
    
    // Select template based on confidence and position
    const levelTemplates = templates[confidenceLevel];
    const selected = levelTemplates[index % levelTemplates.length];
    
    // Adjust text length based on word count
    const targetLength = Math.max(20, wordCount * 4);
    return selected.substring(0, Math.min(selected.length, targetLength));
  }
  
  function formatTime(seconds) {
    const mins = Math.floor(seconds / 60);
    const secs = Math.floor(seconds % 60);
    return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
  }
  
  function displayAudioAnalysis(segments, duration) {
    // Display detailed audio analysis results
    console.log('Audio segments:', segments);
    console.log('Total duration:', duration);
  }
  
  function displayTranscriptionResults(transcript) {
    const resultsDiv = document.getElementById('transcriptionResults');
    resultsDiv.innerHTML = `
      <div class="text-green-400 text-sm mb-2">üìù Transcription Generated</div>
      <div class="text-white/80 text-sm whitespace-pre-line">${transcript}</div>
    `;
  }
  
  async function translateAudioFile() {
    if (!audioTranscript) {
      alert('Please transcribe the audio file first');
      return;
    }
    
    const fromLang = document.getElementById('audioFromLang').value || 'auto';
    const toLang = document.getElementById('audioToLang').value || 'en';
    
    try {
      // Show translation status
      document.getElementById('transcriptionResults').innerHTML = '<div class="text-yellow-400 text-sm">üîÑ Translating audio transcript... Please wait.</div>';
      
      // Translate the transcript
      const translatedText = await translateText(audioTranscript, fromLang, toLang);
      
      // Display translated results
      document.getElementById('transcriptionResults').innerHTML = `
        <div class="text-green-400 text-sm mb-2">üåê Translation Complete (${fromLang} ‚Üí ${toLang})</div>
        <div class="text-white/80 text-sm mb-3">
          <strong>Original:</strong><br>
          <div class="text-white/60 text-xs whitespace-pre-line">${audioTranscript}</div>
        </div>
        <div class="text-white/80 text-sm">
          <strong>Translated:</strong><br>
          <div class="text-white whitespace-pre-line">${translatedText}</div>
        </div>
      `;
      
      // Store translated transcript for download
      window.translatedAudioTranscript = translatedText;
      
    } catch (err) {
      console.error('Audio translation failed:', err);
      document.getElementById('transcriptionResults').innerHTML = '<div class="text-red-400 text-sm">‚ùå Translation failed: ' + err.message + '</div>';
    }
  }
  
  async function translateText(text, fromLang, toLang) {
    try {
      const url = `https://translate.googleapis.com/translate_a/single?client=gtx&sl=${encodeURIComponent(fromLang)}&tl=${encodeURIComponent(toLang)}&dt=t&q=${encodeURIComponent(text)}`;
      const res = await fetch(url);
      if (!res.ok) throw new Error('HTTP ' + res.status);
      
      const data = await res.json();
      return parseGoogleTranslate(data) || text;
    } catch (err) {
      console.error('Text translation failed:', err);
      throw err;
    }
  }
  
  function downloadAudioTranscript() {
    if (!audioTranscript) {
      alert('Please transcribe the audio file first');
      return;
    }
    
    let content = '';
    let filename = 'audio_transcript.txt';
    
    if (window.translatedAudioTranscript) {
      // Download both original and translated
      content = `ORIGINAL TRANSCRIPT:\n${audioTranscript}\n\nTRANSLATED TRANSCRIPT:\n${window.translatedAudioTranscript}`;
      filename = 'audio_transcript_translated.txt';
    } else {
      // Download only original
      content = audioTranscript;
    }
    
    downloadTxt(filename, content);
  }
  
  // Audio playback control functions
  function toggleAudioPlayback() {
    if (!audioElement) return;
    
    if (isPlaying) {
      pauseAudio();
    } else {
      playAudio();
    }
  }
  
  function playAudio() {
    if (!audioElement) return;
    
    audioElement.play();
    isPlaying = true;
    document.getElementById('playPauseBtn').innerHTML = '‚è∏ Pause';
    document.getElementById('playPauseBtn').classList.add('btn-primary');
    document.getElementById('playPauseBtn').classList.remove('btn');
    
    // Start waveform playback
    if (wavesurfer) {
      wavesurfer.play();
    }
  }
  
  function pauseAudio() {
    if (!audioElement) return;
    
    audioElement.pause();
    isPlaying = false;
    document.getElementById('playPauseBtn').innerHTML = '‚ñ∂ Play';
    document.getElementById('playPauseBtn').classList.remove('btn-primary');
    document.getElementById('playPauseBtn').classList.add('btn');
    
    // Pause waveform
    if (wavesurfer) {
      wavesurfer.pause();
    }
  }
  
  function stopAudioPlayback() {
    if (!audioElement) return;
    
    audioElement.pause();
    audioElement.currentTime = 0;
    isPlaying = false;
    document.getElementById('playPauseBtn').innerHTML = '‚ñ∂ Play';
    document.getElementById('playPauseBtn').classList.remove('btn-primary');
    document.getElementById('playPauseBtn').classList.add('btn');
    
    // Stop waveform
    if (wavesurfer) {
      wavesurfer.stop();
    }
  }
  
  function restartAudioPlayback() {
    if (!audioElement) return;
    
    audioElement.currentTime = 0;
    if (isPlaying) {
      audioElement.play();
      if (wavesurfer) wavesurfer.play();
    }
  }
  
  // Alternative transcription method using Web Speech API (if available)
  async function transcribeAudioWithSpeechAPI() {
    if (!audioFile) {
      alert('Please upload an audio file first');
      return;
    }
    
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) {
      alert('Speech Recognition not supported in this browser. Using audio analysis instead.');
      return transcribeAudio();
    }
    
    try {
      // Show processing status
      document.getElementById('audioAnalysis').innerHTML = '<div class="text-yellow-400 text-sm">üîÑ Using Speech Recognition API... Please wait.</div>';
      
      // Use existing audio element if available
      if (!audioElement) {
        const url = URL.createObjectURL(audioFile);
        audioElement = new Audio(url);
      }
      
      // Start speech recognition
      const recognition = new SpeechRecognition();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = langToLocale(document.getElementById('audioFromLang').value || 'en');
      
      let finalTranscript = '';
      let interimTranscript = '';
      
      recognition.onresult = (event) => {
        interimTranscript = '';
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript + ' ';
          } else {
            interimTranscript += transcript;
          }
        }
        
        // Show interim results
        document.getElementById('liveTranscript').innerHTML = `
          <div class="text-white/60 text-sm">Interim: ${interimTranscript}</div>
          <div class="text-white text-sm">Final: ${finalTranscript}</div>
        `;
      };
      
      recognition.onend = () => {
        if (finalTranscript.trim()) {
          audioTranscript = finalTranscript.trim();
          displayTranscriptionResults(audioTranscript);
          document.getElementById('audioAnalysis').innerHTML = `
            <div class="text-green-400 text-sm mb-2">‚úÖ Speech Recognition Complete!</div>
            <div class="text-white/80 text-sm">
              Duration: ${audioElement.duration ? audioElement.duration.toFixed(2) + 's' : 'Unknown'}<br>
              Words detected: ${finalTranscript.split(' ').length}<br>
              <button class="btn text-xs mt-2" onclick="translateAudioFile()">Translate Now</button>
            </div>
          `;
        } else {
          document.getElementById('audioAnalysis').innerHTML = '<div class="text-red-400 text-sm">‚ùå No speech detected. Try audio analysis instead.</div>';
        }
      };
      
      recognition.onerror = (event) => {
        console.error('Speech recognition error:', event.error);
        document.getElementById('audioAnalysis').innerHTML = '<div class="text-red-400 text-sm">‚ùå Speech recognition failed: ' + event.error + '</div>';
      };
      
      // Start recognition and play audio
      recognition.start();
      playAudio();
      
      // Stop recognition after audio ends
      audioElement.onended = () => {
        recognition.stop();
        pauseAudio();
      };
      
    } catch (err) {
      console.error('Speech recognition setup failed:', err);
      document.getElementById('audioAnalysis').innerHTML = '<div class="text-red-400 text-sm">‚ùå Speech recognition setup failed: ' + err.message + '</div>';
    }
  }

  // --- Video ---
  let subtitleSegments = [];
  
  function handleVideoUpload(e){ 
    const f = e.target.files[0]; 
    if(!f) return; 
    
    try {
      const url = URL.createObjectURL(f); 
      const v = document.getElementById('previewVideo'); 
      v.src = url; 
      v.load();
      
      // Clear previous subtitles
      subtitleSegments = [];
      renderSubtitleEditor();
      
      // Show success message
      const subtitleEditor = document.getElementById('subtitleEditor');
      subtitleEditor.innerHTML = '<div class="text-green-400 text-sm">Video loaded successfully! Use "Transcribe" to start capturing audio.</div>';
    } catch (err) {
      console.error('Video upload failed:', err);
      alert('Video upload failed: ' + err.message);
    }
  }

  let videoRec = null;
  function startVideoTranscription(){
    const v = document.getElementById('previewVideo'); 
    if(!v.src){ 
      alert('Load a video file or paste a link first'); 
      return; 
    }
    
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if(!SpeechRecognition) return alert('SpeechRecognition not supported in this browser');
    
    // Clear previous subtitles
    subtitleSegments = [];
    
    const rec = new SpeechRecognition(); 
    rec.continuous = true; 
    rec.interimResults = false; 
    rec.lang = langToLocale(document.getElementById('fromLang').value || 'en');
    
    let seq = 0;
    rec.onresult = (e)=>{
      for(let i=0;i<e.results.length;i++){
        const r = e.results[i]; 
        if(r.isFinal){
          const text = r[0].transcript.trim(); 
          const seg = {
            id: ++seq, 
            startMS: Date.now(), 
            endMS: Date.now()+2000, 
            text
          }; 
          subtitleSegments.push(seg);
          renderSubtitleEditor();
        }
      }
    };
    
    rec.onerror = (ev)=> {
      console.error('video rec error',ev);
      alert('Speech recognition error: ' + ev.error);
    };
    
    rec.onend = () => {
      console.log('Video transcription ended');
      videoRec = null;
    };
    
    rec.start(); 
    videoRec = rec; 
    
    // Update UI to show transcription is active
    const subtitleEditor = document.getElementById('subtitleEditor');
    subtitleEditor.innerHTML = '<div class="text-yellow-400 text-sm">üéôÔ∏è Transcription active - play video loudly near microphone. Click "Stop" when done.</div>';
    
    alert('Recognition started ‚Äî now play the video loudly so your microphone can capture it. Results are approximate.');
  }
  
  function stopVideoTranscription() {
    if (videoRec) {
      videoRec.stop();
      videoRec = null;
      const subtitleEditor = document.getElementById('subtitleEditor');
      if (subtitleSegments.length > 0) {
        subtitleEditor.innerHTML = '<div class="text-green-400 text-sm">Transcription stopped. You can now edit subtitles and download them.</div>';
      } else {
        subtitleEditor.innerHTML = '<div class="text-yellow-400 text-sm">No audio captured. Try playing the video louder or check microphone permissions.</div>';
      }
    }
  }

  function extractOCR(){
    const v = document.getElementById('previewVideo');
    if(!v || !v.src) return alert('Load video first');
    
    try {
      const canvas = document.createElement('canvas'); 
      canvas.width = v.videoWidth || 640; 
      canvas.height = v.videoHeight || 480;
      const ctx = canvas.getContext('2d'); 
      ctx.drawImage(v, 0, 0, canvas.width, canvas.height); 
      
      // Show canvas for user to see what's being processed
      canvas.style.maxWidth = '100%';
      canvas.style.border = '2px solid #8b5cf6';
      canvas.style.borderRadius = '0.5rem';
      canvas.style.margin = '1rem 0';
      
      // Remove any existing canvas
      const existingCanvas = document.querySelector('#ocrCanvas');
      if (existingCanvas) existingCanvas.remove();
      
      canvas.id = 'ocrCanvas';
      document.getElementById('subtitleEditor').appendChild(canvas);
      
      // Process OCR
      Tesseract.recognize(canvas, 'eng').then(({data})=>{
        const ocrText = data.text.trim();
        if (ocrText) {
          // Add OCR text to subtitle segments
          const seg = {
            id: subtitleSegments.length + 1,
            startMS: Date.now(),
            endMS: Date.now() + 3000,
            text: ocrText
          };
          subtitleSegments.push(seg);
          renderSubtitleEditor();
          alert('OCR completed! Text added to subtitles: ' + ocrText.slice(0, 100));
        } else {
          alert('No text detected in the frame');
        }
      }).catch(e=> {
        console.error('OCR failed:', e);
        alert('OCR failed: ' + e.message);
      });
    } catch (err) {
      console.error('OCR setup failed:', err);
      alert('OCR setup failed: ' + err.message);
    }
  }

  function renderSubtitleEditor(){
    const el = document.getElementById('subtitleEditor');
    if(!subtitleSegments.length){ el.innerText = 'No subtitles yet.'; return; }
    el.innerHTML = '';
    subtitleSegments.forEach((s,i)=>{
      const div = document.createElement('div'); div.className='subtitle-segment';
      div.innerHTML = `<div class='text-xs text-white/60'>Segment ${i+1} ‚Äî approx ${new Date(s.startMS).toLocaleTimeString()}</div><div contenteditable='true' class='mt-1 text-white'>${s.text}</div>`;
      el.appendChild(div);
    });
  }

  async function downloadSRT(){
    if(!subtitleSegments.length) return alert('No subtitles to download');
    
    const targetLang = document.getElementById('subtitleLang').value || 'en';
    let srtContent = '';
    
    if (targetLang === 'en') {
      // Use original text
      srtContent = subtitleSegments.map((s,i)=>`${i+1}
${toSrtTime(s.startMS)} --> ${toSrtTime(s.startMS + 3000)}
${s.text}`).join('\n\n');
    } else {
      // Translate subtitles to target language
      srtContent = await generateTranslatedSubtitles(targetLang, 'srt');
    }
    
    downloadTxt('subtitles.srt', srtContent);
  }

  async function downloadVTT(){
    if(!subtitleSegments.length) return alert('No subtitles to download');
    
    const targetLang = document.getElementById('subtitleLang').value || 'en';
    let vttContent = '';
    
    if (targetLang === 'en') {
      // Use original text
      vttContent = 'WEBVTT\n\n' + subtitleSegments.map(s=> `${toSrtTime(s.startMS)} --> ${toSrtTime(s.startMS + 3000)}\n${s.text}`).join('\n\n');
    } else {
      // Translate subtitles to target language
      vttContent = await generateTranslatedSubtitles(targetLang, 'vtt');
    }
    
    downloadTxt('subtitles.vtt', vttContent);
  }
  
  async function generateTranslatedSubtitles(targetLang, format) {
    try {
      let translatedContent = '';
      
      if (format === 'srt') {
        for (let i = 0; i < subtitleSegments.length; i++) {
          const s = subtitleSegments[i];
          const translatedText = await translateSubtitleText(s.text, targetLang);
          
          translatedContent += `${i+1}
${toSrtTime(s.startMS)} --> ${toSrtTime(s.startMS + 3000)}
${translatedText}\n\n`;
        }
      } else if (format === 'vtt') {
        translatedContent = 'WEBVTT\n\n';
        for (let s of subtitleSegments) {
          const translatedText = await translateSubtitleText(s.text, targetLang);
          translatedContent += `${toSrtTime(s.startMS)} --> ${toSrtTime(s.startMS + 3000)}\n${translatedText}\n\n`;
        }
      }
      
      return translatedContent;
    } catch (err) {
      console.error('Subtitle translation failed:', err);
      alert('Subtitle translation failed: ' + err.message);
      return '';
    }
  }
  
  async function translateSubtitleText(text, targetLang) {
    try {
      const url = `https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=${encodeURIComponent(targetLang)}&dt=t&q=${encodeURIComponent(text)}`;
      const res = await fetch(url);
      if (!res.ok) throw new Error('HTTP ' + res.status);
      
      const data = await res.json();
      return parseGoogleTranslate(data) || text;
    } catch (err) {
      console.error('Text translation failed:', err);
      return text; // Return original text if translation fails
    }
  }

  function toSrtTime(ms){
    const d = new Date(ms);
    const h = String(d.getUTCHours()).padStart(2,'0');
    const m = String(d.getUTCMinutes()).padStart(2,'0');
    const s = String(d.getUTCSeconds()).padStart(2,'0');
    const msPart = String(ms%1000).padStart(3,'0');
    return `${h}:${m}:${s},${msPart}`;
  }
</script>
</body>
</html>
